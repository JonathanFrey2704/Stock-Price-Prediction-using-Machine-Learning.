{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TCNandOptuna.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti17BhySjPqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4caeee7d-a1ed-48b6-ce9b-1bd9e34d6530"
      },
      "source": [
        "from google.colab import drive\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.nn.utils import weight_norm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.nn.utils \n",
        "import torch\n",
        "import copy\n",
        "import math\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import mean_squared_error\n",
        "!pip install optuna"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/10/06b58f4120f26b603d905a594650440ea1fd74476b8b360dbf01e111469b/optuna-2.3.0.tar.gz (258kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 16.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 20.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 133kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 143kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 153kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 163kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 174kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 184kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 194kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 204kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 215kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 225kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 235kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 245kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 256kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 5.8MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/c8/c16d30bbed11a1722060014c246d124582d1f781b26f5859d8dacc3e08e1/colorlog-4.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/aa/c261dfd7f4ba6ce4701846a2689a46e2a172e012171de4378fc2926e3bf0/alembic-1.4.3-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 31.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n",
            "Collecting cmaes>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/3c/06c76ec8b54b9b1fad7f35e903fd25010fe3e0d41bd94cea5e6f12e0d651/cmaes-0.7.0-py3-none-any.whl\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.20)\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/61/5b64d73b01c1218f55c894b5ec0fb89b32c6960b7f7b3ad9f5ac0c373b9d/cliff-3.5.0-py3-none-any.whl (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.17.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (20.4)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Collecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 22.4MB/s \n",
            "\u001b[?25hCollecting cmd2!=0.8.3,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/54/af6e2703f064485d717cb311d3f9440cd302a823ba6d80a020b59eae166d/cmd2-1.4.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 25.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.15.0)\n",
            "Collecting PrettyTable<0.8,>=0.7.2\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from stevedore>=2.0.1->cliff->optuna) (2.0.0)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/4c/0b1d507ad7e8bc31d690d04b4f475e74c2002d060f7994ce8c09612df707/pyperclip-1.8.1.tar.gz\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (20.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->stevedore>=2.0.1->cliff->optuna) (3.4.0)\n",
            "Building wheels for collected packages: optuna\n",
            "  Building wheel for optuna (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.3.0-cp36-none-any.whl size=359761 sha256=5b285eb9e50648a8e81ff08fcd0efd209d66104c4cb3ac03dd54a40e0dcc07d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/91/19/64b0ec6b964f89c0695a9dc6db6f851d0b54c5381a5c9cadfb\n",
            "Successfully built optuna\n",
            "Building wheels for collected packages: PrettyTable, pyperclip\n",
            "  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PrettyTable: filename=prettytable-0.7.2-cp36-none-any.whl size=13700 sha256=1b8893c9ea843edb026d3f6e714eaf454f2aca2e0e640354d312a13c52e4f48e\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.1-cp36-none-any.whl size=11119 sha256=4f7e76a8585b7a157b4a04274154dffd411caaf505a18dc51de7fb9df7665a0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/10/3a/c830e9bb3db2c93274ea1f213a41fabde0d8cf3794251fad0c\n",
            "Successfully built PrettyTable pyperclip\n",
            "Installing collected packages: colorlog, python-editor, Mako, alembic, cmaes, pbr, stevedore, colorama, pyperclip, cmd2, PrettyTable, cliff, optuna\n",
            "  Found existing installation: prettytable 2.0.0\n",
            "    Uninstalling prettytable-2.0.0:\n",
            "      Successfully uninstalled prettytable-2.0.0\n",
            "Successfully installed Mako-1.1.3 PrettyTable-0.7.2 alembic-1.4.3 cliff-3.5.0 cmaes-0.7.0 cmd2-1.4.0 colorama-0.4.4 colorlog-4.6.2 optuna-2.3.0 pbr-5.5.1 pyperclip-1.8.1 python-editor-1.0.4 stevedore-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8Ge9acIjXtl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f4b233-b8ef-4057-c52b-0e82b1e1ea5d"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/Colab Notebooks/Data/SPY.csv'\n",
        "\n",
        "df=pd.read_csv(path)\n",
        "print(df)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "                          date   close    high  ...  adjVolume  divCash  splitFactor\n",
            "0    2018-01-02 00:00:00+00:00  268.77  268.81  ...   86655749      0.0          1.0\n",
            "1    2018-01-03 00:00:00+00:00  270.47  270.64  ...   90070416      0.0          1.0\n",
            "2    2018-01-04 00:00:00+00:00  271.61  272.16  ...   80636408      0.0          1.0\n",
            "3    2018-01-05 00:00:00+00:00  273.42  273.56  ...   83523995      0.0          1.0\n",
            "4    2018-01-08 00:00:00+00:00  273.92  274.10  ...   57319192      0.0          1.0\n",
            "..                         ...     ...     ...  ...        ...      ...          ...\n",
            "665  2020-08-24 00:00:00+00:00  342.92  343.00  ...   48588662      0.0          1.0\n",
            "666  2020-08-25 00:00:00+00:00  344.12  344.21  ...   38463381      0.0          1.0\n",
            "667  2020-08-26 00:00:00+00:00  347.57  347.86  ...   50790237      0.0          1.0\n",
            "668  2020-08-27 00:00:00+00:00  348.33  349.90  ...   58034142      0.0          1.0\n",
            "669  2020-08-28 00:00:00+00:00  350.58  350.72  ...   48588940      0.0          1.0\n",
            "\n",
            "[670 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYJID7tcjfFZ"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD8RoXtfjk6v"
      },
      "source": [
        "class Residual_Block(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs,kernel_size=3,stride=(1,1),dilation=2,padding = 0, \n",
        "               dropout = 0.2, weight_normalization=True):\n",
        "    \n",
        "        super(Residual_Block, self).__init__()\n",
        "        \n",
        "        self.n_inputs = n_inputs #input channel\n",
        "        self.n_outputs = n_outputs #output channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        \n",
        "        \n",
        "        self.conv1 = nn.Conv1d(in_channels = n_inputs,out_channels = n_outputs,\n",
        "                               kernel_size = kernel_size,stride = stride, padding = padding, dilation =dilation)\n",
        "        self.activation1 =nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.conv2 = nn.Conv1d(in_channels = n_outputs,out_channels = n_outputs,\n",
        "                               kernel_size = kernel_size,stride = stride, padding = padding, dilation=dilation)\n",
        "        self.activation2 =nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        \n",
        "        if weight_normalization:\n",
        "            self.conv1 = nn.utils.weight_norm(self.conv1)\n",
        "            self.conv2 = nn.utils.weight_norm(self.conv2)\n",
        "            \n",
        "        if n_inputs != n_outputs:\n",
        "            self.downsample = nn.Conv1d(n_inputs,n_outputs,1)\n",
        "            \n",
        "        self.activation3 = nn.ReLU()\n",
        "        \n",
        "    #padding left of data since pytorch only padds bi-directionally\n",
        "    def forward(self, x):\n",
        "        residuals = x\n",
        "        \n",
        "        #padding for first convnet\n",
        "        left_padding = ((self.dilation)*(self.kernel_size-1))\n",
        "        pad = torch.from_numpy(np.zeros(left_padding*x.shape[0]*x.shape[1]).reshape(x.shape[0],self.n_inputs,left_padding).astype('float32'))\n",
        "        x = torch.cat((pad,x),2) #concat along input width aka time\n",
        "        \n",
        "        output = self.conv1(x)\n",
        "        output = self.activation1(output)\n",
        "        output = self.dropout1(output)\n",
        "     \n",
        "        \n",
        "        #print('conv 2')\n",
        "        #print('original input shape: ', output.shape)\n",
        "        #conv2\n",
        "        left_padding2 = ((self.dilation)*(self.kernel_size-1))#(2**self.dilation*(self.kernel_size-1)) #<- original\n",
        "        #need extra padding across out channels\n",
        "        pad2 = torch.from_numpy(np.zeros(left_padding2*output.shape[0]*output.shape[1]).reshape(output.shape[0]\n",
        "                                                                    ,self.n_outputs,left_padding2).astype('float32'))\n",
        "\n",
        "        output = torch.cat((pad2,output),2) #concat along input width aka time     \n",
        "        \n",
        "        output = self.conv2(output)\n",
        "        output = self.activation2(output)\n",
        "        output = self.dropout2(output)\n",
        "        \n",
        "        #if input channels and output channels are of different size use a 1D conv with kernel 1    \n",
        "\n",
        "        if self.n_inputs != self.n_outputs:\n",
        "            residuals = self.downsample(residuals)\n",
        "\n",
        "        return self.activation3(output + residuals)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgfCyQ9gjni3"
      },
      "source": [
        "class TemporalConvolutionalNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.2, weight_norm = True):\n",
        "        super(TemporalConvolutionalNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels= len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2**i\n",
        "            if i==0:\n",
        "                input_channels = num_inputs\n",
        "            else:\n",
        "                input_channels = num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers+=[Residual_Block(n_inputs = input_channels, n_outputs = out_channels,kernel_size = kernel_size,\n",
        "                                    stride=1, dilation = dilation_size,padding = 0, dropout = dropout)]\n",
        "        print('layers')\n",
        "        for j in layers:\n",
        "            print(j.n_inputs,j.n_outputs)\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnxB0jDx69Xl"
      },
      "source": [
        "'''\n",
        "RSI = 100 – 100 / ( 1 + RS )\n",
        "RS = Relative Strength = Avg_gain / Avg_Loss\n",
        "Avg_gain = average of all up moves in the last N price \n",
        "Avg_Loss = average of all down moves in the last N price \n",
        "N = the window of RSI\n",
        "'''\n",
        "#taken for stack overflow dont forget to put credits \n",
        "def average_gainloss(series, window_size, average):\n",
        "    a = (window_size-1) / window_size\n",
        "    ak = a**np.arange(len(series)-1, -1, -1) #arange in descending order\n",
        "    return np.append(average, np.cumsum(ak * series) / ak / window_size + average * a**np.arange(1, len(series)+1))\n",
        "\n",
        "\n",
        "def rsi(df, window_size = 14):\n",
        "\n",
        "    df['change'] = df['close'].diff()\n",
        "    df['gain'] = df.change.mask(df.change < 0, 0.0)\n",
        "    df['loss'] = -df.change.mask(df.change > 0, -0.0)\n",
        "    df.loc[window_size:,'avg_gain'] = average_gainloss( df.gain[window_size+1:].values, window_size, df.loc[:window_size, 'gain'].mean())\n",
        "    df.loc[window_size:,'avg_loss'] = average_gainloss( df.loss[window_size+1:].values, window_size, df.loc[:window_size, 'loss'].mean())\n",
        "    df['rs'] = df.avg_gain / df.avg_loss\n",
        "    df['rsi'] = 100 - (100 / (1 + df.rs))\n",
        "    \n",
        "    return df\n",
        "   "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZMbY8BaOFXm"
      },
      "source": [
        "def mulitvariate_preprocessing(num_features,time_steps, df, num_predictions, sampling_method):\n",
        "    '''\n",
        "    Ex:\n",
        "    num_predictions = 2\n",
        "    input : t1,t2,t3,t4,t5\n",
        "    target : t3,t4,t5,t6,t7\n",
        "    predictions = t6,t7\n",
        "    sampling method: either you push the sliding window along the data or you jump ie select sample [0,timestep],[timestep,2*timstep] etc..\n",
        "    \n",
        "    '''  \n",
        "    \n",
        "    sampling_index = 0\n",
        "    end_sampling_index = time_steps\n",
        "\n",
        "    #set final subsampling shape Temp array to append actual data in the right shape\n",
        "    temp = np.arange(time_steps) \n",
        "    for i in range(num_features-1):\n",
        "        temp = np.vstack((temp,np.arange(time_steps))) #end of for loop you get 4 depth,20  input length shape\n",
        "    temp = np.vstack(([temp],[temp])) #shape (2,4,20) or (samples, channels/features, time length)\n",
        "\n",
        "\n",
        "    train_Set = df.iloc[sampling_index:end_sampling_index, [0]].to_numpy().flatten() #first time_steps samples\n",
        "    #print(train_Set)\n",
        "        \n",
        "    if sampling_method == 'sliding_window':\n",
        "        \n",
        "        \n",
        "        target_start = num_predictions  #retrieve t+1\n",
        "        target_end = target_start + time_steps\n",
        "        y_train = df.iloc[target_start:target_end, [0]].to_numpy().flatten() #first target samples\n",
        "\n",
        "        \n",
        "        for i in range((df.shape[0]-num_predictions-time_steps)+num_predictions): #loop of data to generate samples\n",
        "  \n",
        "            for i in range(1,num_features): #loop to iterate over features for x\n",
        "               \n",
        "                train_Set = np.vstack((train_Set,\n",
        "                                   df.iloc[sampling_index:end_sampling_index, [i]].to_numpy().flatten()))\n",
        "                \n",
        "            #print('x',train_Set[0:1,])\n",
        "            \n",
        "            sampling_index += num_predictions\n",
        "            end_sampling_index += num_predictions\n",
        "            \n",
        "            target_start += num_predictions\n",
        "            target_end += num_predictions\n",
        "            \n",
        "                \n",
        "            temp = np.vstack((temp,[train_Set])) #shape n, feature,time length\n",
        "            train_Set = df.iloc[sampling_index:end_sampling_index, [0]].to_numpy().flatten() #reset to get shape 20, -> array of sub-sampled prices\n",
        "            \n",
        "            if target_end > df.shape[0]:\n",
        "                break #error checking condition\n",
        "            #y-value\n",
        "            y_train = np.vstack((y_train,\n",
        "                                        df.iloc[target_start:target_end, [0]].to_numpy().flatten()))\n",
        "\n",
        "        y_train = y_train.reshape(-1,1,time_steps).astype('float32') #reshape to sample, 1 ,timesteps\n",
        "            \n",
        "            \n",
        "    else:\n",
        "        #jumping by timesteps\n",
        "        \n",
        "        for i in range(int(df.shape[0])//time_steps):\n",
        "            for i in range(1,num_features): #loop to iterate over features for x\n",
        "\n",
        "                train_Set = np.vstack((train_Set,\n",
        "                                       df.iloc[sampling_index:end_sampling_index, [i]].to_numpy().flatten()))\n",
        "                \n",
        "            #print('x',train_Set[0:1,])\n",
        "            sampling_index += time_steps\n",
        "            end_sampling_index += time_steps\n",
        "            \n",
        "            temp = np.vstack((temp,[train_Set])) #shape n, feature,time length\n",
        "            train_Set = df.iloc[sampling_index:end_sampling_index, [0]].to_numpy().flatten() #reset to get shape 20, -> array of sub-sampled prices\n",
        "    \n",
        "\n",
        "        y_train = df['close'].tail(df.shape[0]-num_predictions) # last n - num_prediction values\n",
        "        y_train = y_train.to_numpy().flatten().reshape(-1,1,time_steps).astype('float32')    \n",
        "        \n",
        "   \n",
        "    \n",
        "    print('Finish resampling')\n",
        "    \n",
        "    temp = temp[2:,:,:].astype('float32')\n",
        "      \n",
        "    #print('x',temp.shape)#nice shape is good\n",
        "    #print(temp[:1,:,:])#check subsample for proper format\n",
        "    #print('y',y_train.shape)\n",
        "    \n",
        "    X_train_torch = torch.from_numpy(temp)\n",
        "    \n",
        "    y__train_torch = torch.from_numpy(y_train) # n_Samples x 1 x temporal length\n",
        "    \n",
        "    return X_train_torch, y__train_torch"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERP2d3uzCdz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0fb611-5c8d-4cf8-ff5c-9efa7d8313c3"
      },
      "source": [
        "############# Any forecast size >1 is multi-step/ horizon forecasting. due to the nature of the data\n",
        "############## And the volatility this can lead to unaccurate predictions, therefore we need a recursive forecasting\n",
        "###Method as well for when forecast_size = 5\n",
        "window_size = 14\n",
        "time_steps= 20 #input length\n",
        "forecast_size = 1  #shift in y target as well\n",
        "np.set_printoptions(suppress=True)\n",
        "training_sample_size = 400\n",
        "validation_sample_size = 100\n",
        "test_sample_size = 50 #for sliding window this is the number of sample you can predict, test_sample_size/forecast_size\n",
        "#ie for sliding window size if forecast_size =1 -> there are 10 extra sample size, if slioding window = 2, there would be 5 extra samples\n",
        "#for jump, this would need to be a multiple of timesteps\n",
        "sampling_Style = 'sliding_window' # sliding_window or jump\n",
        "\n",
        "\n",
        "sample = df[['date','close']][:660] #need to ensure extra data for y-target series forecasting\n",
        "sample['date']= pd.to_datetime(sample['date'], format ='%Y-%m-%d %H:%M:%S' ) #convert string to date time\n",
        "sample['dayofweek']= sample['date'].dt.dayofweek\n",
        "sample['std'] = sample['close'].pct_change()*100\n",
        "\n",
        "sample_rsi = rsi(sample, window_size)\n",
        "\n",
        "sample_rsi = sample_rsi.set_index('date')\n",
        "start_index =((sample.shape[0]-window_size)%time_steps + window_size)\n",
        "sample_rsi = sample_rsi[start_index:]#cutoff NaN\n",
        "sample_rsi = sample_rsi[['close','dayofweek','rsi','std']]  # predicting feature should be in column index 0\n",
        "\n",
        "num_features = sample_rsi.shape[1]\n",
        "\n",
        "training_cutoff = training_sample_size + forecast_size \n",
        "validation_cutoff = time_steps + training_cutoff + validation_sample_size + forecast_size\n",
        "testing_cutoff = time_steps + validation_cutoff  + forecast_size\n",
        "forecast_testing_cut = testing_cutoff + test_sample_size\n",
        "\n",
        "train = sample_rsi[:training_cutoff]\n",
        "val = sample_rsi[training_cutoff:validation_cutoff]\n",
        "testing = sample_rsi[validation_cutoff:testing_cutoff]\n",
        "forecast = sample_rsi[validation_cutoff:forecast_testing_cut]\n",
        "\n",
        "\n",
        "X_train, y_train = mulitvariate_preprocessing(num_features,time_steps,train,forecast_size,'sliding_window' ) #or jump\n",
        "\n",
        "X_val, y_val = mulitvariate_preprocessing(num_features,time_steps,val, forecast_size,'sliding_window')\n",
        "\n",
        "X_test, y_test = mulitvariate_preprocessing(num_features,time_steps,testing, forecast_size,'sliding_window')\n",
        "\n",
        "X_fore, y_fore = mulitvariate_preprocessing(num_features,time_steps,forecast, forecast_size,'sliding_window')\n",
        "\n",
        "\n",
        "print('Test data size, x-shape, y - shape: ', training_cutoff, X_train.shape, y_train.shape)\n",
        "print('Validation data size, x-shape, y - shape: ',  validation_cutoff - training_cutoff, X_val.shape, y_val.shape )\n",
        "print('Test data size, x-shape, y - shape: ', testing_cutoff - validation_cutoff, X_test.shape, y_test.shape )\n",
        "print('Forecast data size, x-shape, y - shape: ', forecast_testing_cut - validation_cutoff, X_fore.shape, y_fore.shape )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish resampling\n",
            "Finish resampling\n",
            "Finish resampling\n",
            "Finish resampling\n",
            "Test data size, x-shape, y - shape:  401 torch.Size([381, 4, 20]) torch.Size([381, 1, 20])\n",
            "Validation data size, x-shape, y - shape:  121 torch.Size([101, 4, 20]) torch.Size([101, 1, 20])\n",
            "Test data size, x-shape, y - shape:  21 torch.Size([1, 4, 20]) torch.Size([1, 1, 20])\n",
            "Forecast data size, x-shape, y - shape:  71 torch.Size([51, 4, 20]) torch.Size([51, 1, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5POUBBwR5XjS",
        "outputId": "cc7dcf34-19b0-42b5-f743-2fa80dc2ae23"
      },
      "source": [
        "in_channels = num_features\n",
        "out_channel = 1\n",
        "filter_size = 3\n",
        "#receptive field = 1 + 2*(kernel_size -1)*(dilation_base^n -1) where n is the nth layer, so we need to solve for n\n",
        "num_layers = math.ceil(math.log((X_train.shape[2]-1)/(2*(filter_size - 1))+1,2))\n",
        "channels = [2,out_channel]\n",
        "print('Current layers', len(channels))\n",
        "if len(channels) != num_layers:\n",
        "    print('You should not have more than {} residual layers or else model will be equivalent to RNN'.format(num_layers))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current layers 2\n",
            "You should not have more than 3 residual layers or else model will be equivalent to RNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef6gc-fb5its"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gXIbsOJ7eLc"
      },
      "source": [
        "import optuna"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eX5Ott57hat"
      },
      "source": [
        "def objective(trial):\n",
        " \n",
        "  model = TemporalConvolutionalNet(in_channels,channels,kernel_size = filter_size,dropout=0.2,weight_norm=True)\n",
        "  # Invoke suggest methods of a Trial object to generate hyperparameters.\n",
        "  optimizer_name = trial.suggest_categorical('optimizer', ['AdamW', 'Adam', 'RMSprop', 'Rprop', 'ASGD'])\n",
        "  # Loguniform parameter\n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
        "  optimizer = getattr(torch.optim,optimizer_name)(model.parameters(),lr=learning_rate)\n",
        "\n",
        "  loss = torch.nn.MSELoss()\n",
        "  \n",
        "  model.train()\n",
        "  num_epoch = 500\n",
        "  X_train, y_train = mulitvariate_preprocessing(num_features,time_steps,train,forecast_size,'sliding_window' ) #or jump\n",
        "  X_val, y_val = mulitvariate_preprocessing(num_features,time_steps,val, forecast_size,'sliding_window')\n",
        "  for epoch in range(1, num_epoch):\n",
        "    y_pred = model(X_train)                   # Make predictions (final-layer activations)\n",
        "    loss_value = loss(y_pred, y_train)                 # Compute loss with respect to predictions\n",
        "    #print(y_pred)\n",
        "    \n",
        "    model.zero_grad()                   # Reset all gradient accumulators to zero (PyTorch thing)\n",
        "    loss_value.backward()                        # Compute gradient of loss wrt all parameters (backprop!)\n",
        "    optimizer.step()                    # Use the gradients to take a step with SGD.\n",
        "        \n",
        "    #print(\"Epoch %d final had loss %.4f\" % (epoch+1, loss_value.item()))\n",
        "    model.eval()\n",
        "    \n",
        "    validation_loss = []\n",
        "    with torch.no_grad():\n",
        "      model.eval() #stops dropout\n",
        "      for i in range(X_val.shape[0]):\n",
        "        x_input = X_val[i:i+1,:,:]\n",
        "        y_target = y_val[i:i+1,:,:]\n",
        "        y_hat = model(x_input)                   \n",
        "        val_loss = loss(y_hat, y_target)\n",
        "        validation_loss.append(val_loss.item())\n",
        "  print('average validaiton loss :', np.mean(np.array(validation_loss))) \n",
        "    #return error # An objective value linked with the Trial object.\n",
        "  return np.mean(np.array(validation_loss))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2SOpprK95Ki",
        "outputId": "78aacc46-6f2c-494c-c82d-5b3ec4c7e8ad"
      },
      "source": [
        "study = optuna.create_study()  # Create a new study.\n",
        "study.optimize(objective, n_trials=10)  # Invoke optimization of the objective function.\n",
        "#print(study.best_params)\n",
        "print(study.best_trial)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-06 05:27:07,930]\u001b[0m A new study created in memory with name: no-name-f5e9dc09-d1b8-4d7c-be48-9de347c54f33\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "layers\n",
            "4 2\n",
            "2 1\n",
            "Finish resampling\n",
            "Finish resampling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-06 05:27:55,536]\u001b[0m Trial 0 finished with value: 98274.50742574257 and parameters: {'optimizer': 'Adam', 'learning_rate': 1.3468467309151396e-05}. Best is trial 0 with value: 98274.50742574257.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average validaiton loss : 98274.50742574257\n",
            "layers\n",
            "4 2\n",
            "2 1\n",
            "Finish resampling\n",
            "Finish resampling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-06 05:28:45,377]\u001b[0m Trial 1 finished with value: 1401.8928769555423 and parameters: {'optimizer': 'Rprop', 'learning_rate': 0.0001601334489247165}. Best is trial 1 with value: 1401.8928769555423.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average validaiton loss : 1401.8928769555423\n",
            "layers\n",
            "4 2\n",
            "2 1\n",
            "Finish resampling\n",
            "Finish resampling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-06 05:29:32,189]\u001b[0m Trial 2 finished with value: 47725.66932240099 and parameters: {'optimizer': 'RMSprop', 'learning_rate': 5.7540710500445846e-05}. Best is trial 1 with value: 1401.8928769555423.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average validaiton loss : 47725.66932240099\n",
            "layers\n",
            "4 2\n",
            "2 1\n",
            "Finish resampling\n",
            "Finish resampling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-06 05:30:19,503]\u001b[0m Trial 3 finished with value: 514.6020764643603 and parameters: {'optimizer': 'Adam', 'learning_rate': 0.000951901678033251}. Best is trial 3 with value: 514.6020764643603.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average validaiton loss : 514.6020764643603\n",
            "layers\n",
            "4 2\n",
            "2 1\n",
            "Finish resampling\n",
            "Finish resampling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-06 05:31:07,187]\u001b[0m Trial 4 finished with value: 42.606706808109095 and parameters: {'optimizer': 'Adam', 'learning_rate': 0.004907022477892957}. Best is trial 4 with value: 42.606706808109095.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average validaiton loss : 42.606706808109095\n",
            "layers\n",
            "4 2\n",
            "2 1\n",
            "Finish resampling\n",
            "Finish resampling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-06 05:31:55,666]\u001b[0m Trial 5 finished with value: 1401.8950174161703 and parameters: {'optimizer': 'Rprop', 'learning_rate': 0.0005344924007554208}. Best is trial 4 with value: 42.606706808109095.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average validaiton loss : 1401.8950174161703\n",
            "layers\n",
            "4 2\n",
            "2 1\n",
            "Finish resampling\n",
            "Finish resampling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-06 05:32:44,964]\u001b[0m Trial 6 finished with value: 9.02268945344604 and parameters: {'optimizer': 'Rprop', 'learning_rate': 0.004699301199851891}. Best is trial 6 with value: 9.02268945344604.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average validaiton loss : 9.02268945344604\n",
            "layers\n",
            "4 2\n",
            "2 1\n",
            "Finish resampling\n",
            "Finish resampling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-06 05:33:32,080]\u001b[0m Trial 7 finished with value: 97760.66947710396 and parameters: {'optimizer': 'AdamW', 'learning_rate': 0.000985532757552901}. Best is trial 6 with value: 9.02268945344604.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average validaiton loss : 97760.66947710396\n",
            "layers\n",
            "4 2\n",
            "2 1\n",
            "Finish resampling\n",
            "Finish resampling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-06 05:34:20,022]\u001b[0m Trial 8 finished with value: 3657.1866660543005 and parameters: {'optimizer': 'AdamW', 'learning_rate': 0.00026896670576222854}. Best is trial 6 with value: 9.02268945344604.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average validaiton loss : 3657.1866660543005\n",
            "layers\n",
            "4 2\n",
            "2 1\n",
            "Finish resampling\n",
            "Finish resampling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2020-12-06 05:35:06,987]\u001b[0m Trial 9 finished with value: 98274.50742574257 and parameters: {'optimizer': 'AdamW', 'learning_rate': 0.003195185109271419}. Best is trial 6 with value: 9.02268945344604.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average validaiton loss : 98274.50742574257\n",
            "FrozenTrial(number=6, value=9.02268945344604, datetime_start=datetime.datetime(2020, 12, 6, 5, 31, 55, 667150), datetime_complete=datetime.datetime(2020, 12, 6, 5, 32, 44, 964086), params={'optimizer': 'Rprop', 'learning_rate': 0.004699301199851891}, distributions={'optimizer': CategoricalDistribution(choices=('AdamW', 'Adam', 'RMSprop', 'Rprop', 'ASGD')), 'learning_rate': LogUniformDistribution(high=0.01, low=1e-05)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=6, state=TrialState.COMPLETE)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IAe2VhbkuQg"
      },
      "source": [
        "#optimizer': 'Rprop', 'learning_rate': 0.0004053772530651773"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghks11lMDT4L",
        "outputId": "0c9581c6-c373-467e-e8a8-c8839b0a6c21"
      },
      "source": [
        "# The number of times to evaluate the full training data (in this case, number of gradient steps)\n",
        "num_epoch = 500\n",
        "model_ = TemporalConvolutionalNet(in_channels,channels,kernel_size = filter_size,dropout=0.2,weight_norm=True)\n",
        "loss = torch.nn.MSELoss()\n",
        "# Use stochastic gradient descent to train the model\n",
        "optimizer = torch.optim.Rprop(model_.parameters(), lr=0.004699301199851891)\n",
        "'''\n",
        "########################################################## \n",
        "beta is first and second order moments, basically the speed and momentum \n",
        "the sharpness of the the movements and how far the move(shifting of the curve up and down)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.035, betas=(0.35,0.25), weight_decay=0.25)\n",
        "layers\n",
        "4 2\n",
        "2 1\n",
        "'''\n",
        "for epoch in range(1, num_epoch): \n",
        "    \n",
        "  y_pred = model_(X_train)                   # Make predictions (final-layer activations)\n",
        "  loss_value = loss(y_pred, y_train)                 # Compute loss with respect to predictions\n",
        "  #print(y_pred)\n",
        "\n",
        "  model_.zero_grad()                   # Reset all gradient accumulators to zero (PyTorch thing)\n",
        "  loss_value.backward()                        # Compute gradient of loss wrt all parameters (backprop!)\n",
        "  optimizer.step()                    # Use the gradients to take a step with SGD.\n",
        "        \n",
        "print(\"Epoch %d final had loss %.4f\" % (epoch+1, loss_value.item()))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "layers\n",
            "4 2\n",
            "2 1\n",
            "Epoch 500 final minibatch had loss 6.7009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "zqJK5uHzlUJU",
        "outputId": "eebe5f99-2042-461e-c27a-ff81468cbe8c"
      },
      "source": [
        "validation_loss = []\n",
        "with torch.no_grad():\n",
        "    model_.eval() #stops dropout\n",
        "    for i in range(X_val.shape[0]):\n",
        "        x_input = X_val[i:i+1,:,:]\n",
        "        y_target = y_val[i:i+1,:,:]\n",
        "        \n",
        "        y_hat = model_(x_input)                   \n",
        "        val_loss = loss(y_hat, y_target)\n",
        "        validation_loss.append(val_loss.item())\n",
        "        \n",
        "print('average validaiton loss :', np.mean(np.array(validation_loss)))\n",
        "plt.plot(np.arange(len(validation_loss)),validation_loss,label='Loss')\n",
        "plt.title('Validation Loss')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average validaiton loss : 11.051003026490164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Validation Loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1qd5_XJlX7R",
        "outputId": "2d719851-099d-495f-b079-2bc7ef3f4b63"
      },
      "source": [
        "model_.eval()\n",
        "forecasted = []\n",
        "for sample in range(X_fore.shape[0]):\n",
        "    sample_input = X_fore[sample:sample+1,:,:]\n",
        "    y_pred_fore = model_(sample_input)\n",
        "    print(X_fore[sample:sample+1,0:1,:])\n",
        "    print(y_pred_fore)\n",
        "    y_pred_fore_val = y_pred_fore.detach().numpy().reshape(-1) # entirety of model ouput in 1D array\n",
        "    y_fore_hat = y_pred_fore_val[y_pred_fore.shape[2]-forecast_size:] # forcasted aka shifted y, take last forecast_size elements\n",
        "    forecasted.append(y_fore_hat[0])\n",
        "    print(y_fore_hat)\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[296.2600, 309.0900, 300.2400, 312.8600, 302.4600, 297.4600, 274.2300,\n",
            "          288.4200, 274.3600, 248.1100, 269.3200, 239.8500, 252.8000, 240.0000,\n",
            "          240.5100, 228.8000, 222.9500, 243.1500, 246.7900, 261.2000]]])\n",
            "tensor([[[295.5352, 308.2050, 298.6119, 309.7364, 299.9129, 295.0651, 275.6481,\n",
            "          286.4993, 274.2622, 251.4241, 268.1603, 243.0930, 256.1805, 242.5055,\n",
            "          242.9831, 231.4538, 228.4230, 244.3334, 248.6367, 261.2863]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[261.28625]\n",
            "tensor([[[309.0900, 300.2400, 312.8600, 302.4600, 297.4600, 274.2300, 288.4200,\n",
            "          274.3600, 248.1100, 269.3200, 239.8500, 252.8000, 240.0000, 240.5100,\n",
            "          228.8000, 222.9500, 243.1500, 246.7900, 261.2000, 253.4200]]])\n",
            "tensor([[[308.2106, 297.9893, 309.8205, 299.9129, 295.0651, 275.6481, 286.4993,\n",
            "          274.2091, 251.4241, 268.1603, 243.0930, 256.1805, 242.5055, 242.9831,\n",
            "          231.4538, 228.4230, 244.3334, 248.6367, 261.2863, 254.6913]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[254.69133]\n",
            "tensor([[[300.2400, 312.8600, 302.4600, 297.4600, 274.2300, 288.4200, 274.3600,\n",
            "          248.1100, 269.3200, 239.8500, 252.8000, 240.0000, 240.5100, 228.8000,\n",
            "          222.9500, 243.1500, 246.7900, 261.2000, 253.4200, 261.6500]]])\n",
            "tensor([[[299.8899, 309.4746, 300.3780, 295.0651, 275.6481, 286.4993, 274.2091,\n",
            "          251.3673, 268.1603, 243.0930, 256.1805, 242.5055, 242.9831, 231.4538,\n",
            "          228.4230, 244.3334, 248.6367, 261.2863, 254.6913, 263.7888]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[263.78885]\n",
            "tensor([[[312.8600, 302.4600, 297.4600, 274.2300, 288.4200, 274.3600, 248.1100,\n",
            "          269.3200, 239.8500, 252.8000, 240.0000, 240.5100, 228.8000, 222.9500,\n",
            "          243.1500, 246.7900, 261.2000, 253.4200, 261.6500, 257.7500]]])\n",
            "tensor([[[309.2566, 299.9129, 295.0651, 275.6481, 286.4993, 274.2091, 251.3673,\n",
            "          268.1034, 243.0930, 256.1805, 242.5055, 242.9831, 231.4538, 228.4230,\n",
            "          244.3334, 248.6367, 261.2863, 254.6913, 263.7888, 259.1422]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[259.1422]\n",
            "tensor([[[302.4600, 297.4600, 274.2300, 288.4200, 274.3600, 248.1100, 269.3200,\n",
            "          239.8500, 252.8000, 240.0000, 240.5100, 228.8000, 222.9500, 243.1500,\n",
            "          246.7900, 261.2000, 253.4200, 261.6500, 257.7500, 246.1500]]])\n",
            "tensor([[[299.9129, 294.8683, 275.6164, 286.4393, 274.2091, 251.3673, 268.1034,\n",
            "          243.0361, 256.1805, 242.5055, 242.9831, 231.4538, 228.4230, 244.3334,\n",
            "          248.6367, 261.2863, 254.6913, 263.7888, 259.1422, 248.5744]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[248.57436]\n",
            "tensor([[[297.4600, 274.2300, 288.4200, 274.3600, 248.1100, 269.3200, 239.8500,\n",
            "          252.8000, 240.0000, 240.5100, 228.8000, 222.9500, 243.1500, 246.7900,\n",
            "          261.2000, 253.4200, 261.6500, 257.7500, 246.1500, 251.8300]]])\n",
            "tensor([[[294.8683, 276.0727, 286.4959, 274.3163, 251.3673, 268.1034, 243.0361,\n",
            "          256.1237, 242.5055, 242.9831, 231.4538, 228.4230, 244.3334, 248.6367,\n",
            "          261.2863, 254.6913, 263.7888, 259.1422, 248.5744, 252.9003]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[252.90025]\n",
            "tensor([[[274.2300, 288.4200, 274.3600, 248.1100, 269.3200, 239.8500, 252.8000,\n",
            "          240.0000, 240.5100, 228.8000, 222.9500, 243.1500, 246.7900, 261.2000,\n",
            "          253.4200, 261.6500, 257.7500, 246.1500, 251.8300, 248.1900]]])\n",
            "tensor([[[277.1593, 286.6308, 274.5716, 251.3673, 268.1034, 243.0361, 256.1237,\n",
            "          242.4487, 242.9831, 231.4538, 228.4230, 244.3334, 248.6367, 261.2863,\n",
            "          254.6913, 263.7888, 259.1422, 248.5744, 252.9003, 249.7219]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[249.72186]\n",
            "tensor([[[288.4200, 274.3600, 248.1100, 269.3200, 239.8500, 252.8000, 240.0000,\n",
            "          240.5100, 228.8000, 222.9500, 243.1500, 246.7900, 261.2000, 253.4200,\n",
            "          261.6500, 257.7500, 246.1500, 251.8300, 248.1900, 264.8600]]])\n",
            "tensor([[[287.1509, 273.9225, 251.5843, 268.1034, 243.0361, 256.1237, 242.4487,\n",
            "          242.9262, 231.4538, 228.4230, 244.3334, 248.6367, 261.2863, 254.6913,\n",
            "          263.7888, 259.1422, 248.5744, 252.9003, 249.7219, 265.1628]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[265.16284]\n",
            "tensor([[[274.3600, 248.1100, 269.3200, 239.8500, 252.8000, 240.0000, 240.5100,\n",
            "          228.8000, 222.9500, 243.1500, 246.7900, 261.2000, 253.4200, 261.6500,\n",
            "          257.7500, 246.1500, 251.8300, 248.1900, 264.8600, 265.1300]]])\n",
            "tensor([[[273.8667, 249.5521, 267.9295, 242.6546, 256.1237, 242.4487, 242.9262,\n",
            "          231.3969, 228.4230, 244.3334, 248.6367, 261.2863, 254.6913, 263.7888,\n",
            "          259.1422, 248.5744, 252.9003, 249.7219, 265.1628, 265.9382]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[265.93817]\n",
            "tensor([[[248.1100, 269.3200, 239.8500, 252.8000, 240.0000, 240.5100, 228.8000,\n",
            "          222.9500, 243.1500, 246.7900, 261.2000, 253.4200, 261.6500, 257.7500,\n",
            "          246.1500, 251.8300, 248.1900, 264.8600, 265.1300, 274.0300]]])\n",
            "tensor([[[249.5521, 267.9295, 242.6546, 256.1237, 242.4487, 242.9262, 231.3969,\n",
            "          228.3662, 244.3334, 248.6367, 261.2863, 254.6913, 263.7888, 259.1422,\n",
            "          248.5744, 252.9003, 249.7219, 265.1628, 265.9382, 273.9287]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[273.92874]\n",
            "tensor([[[269.3200, 239.8500, 252.8000, 240.0000, 240.5100, 228.8000, 222.9500,\n",
            "          243.1500, 246.7900, 261.2000, 253.4200, 261.6500, 257.7500, 246.1500,\n",
            "          251.8300, 248.1900, 264.8600, 265.1300, 274.0300, 278.2000]]])\n",
            "tensor([[[267.9295, 242.6546, 256.1237, 242.4487, 242.9262, 231.3969, 228.3662,\n",
            "          244.3334, 248.6367, 261.2863, 254.6913, 263.7888, 259.1422, 248.5744,\n",
            "          252.9003, 249.7219, 265.1628, 265.9382, 273.9287, 277.9310]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[277.93103]\n",
            "tensor([[[239.8500, 252.8000, 240.0000, 240.5100, 228.8000, 222.9500, 243.1500,\n",
            "          246.7900, 261.2000, 253.4200, 261.6500, 257.7500, 246.1500, 251.8300,\n",
            "          248.1900, 264.8600, 265.1300, 274.0300, 278.2000, 275.6600]]])\n",
            "tensor([[[243.2043, 253.2637, 242.2272, 242.2365, 231.3969, 228.3662, 244.3334,\n",
            "          248.6367, 261.2863, 254.6913, 263.7888, 259.1422, 248.5744, 252.9003,\n",
            "          249.7219, 265.1628, 265.9382, 273.9287, 277.9310, 276.1101]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[276.1101]\n",
            "tensor([[[252.8000, 240.0000, 240.5100, 228.8000, 222.9500, 243.1500, 246.7900,\n",
            "          261.2000, 253.4200, 261.6500, 257.7500, 246.1500, 251.8300, 248.1900,\n",
            "          264.8600, 265.1300, 274.0300, 278.2000, 275.6600, 283.7900]]])\n",
            "tensor([[[253.2469, 242.1430, 242.2365, 231.3969, 228.3662, 244.3334, 248.6367,\n",
            "          261.2863, 254.6913, 263.7888, 259.1422, 248.5744, 252.9003, 249.7219,\n",
            "          265.1628, 265.9382, 273.9287, 277.9310, 276.1101, 283.3156]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[283.31558]\n",
            "tensor([[[240.0000, 240.5100, 228.8000, 222.9500, 243.1500, 246.7900, 261.2000,\n",
            "          253.4200, 261.6500, 257.7500, 246.1500, 251.8300, 248.1900, 264.8600,\n",
            "          265.1300, 274.0300, 278.2000, 275.6600, 283.7900, 277.7600]]])\n",
            "tensor([[[242.1430, 241.9978, 231.3969, 228.2963, 244.3334, 248.6367, 261.2863,\n",
            "          254.6913, 263.7888, 259.1422, 248.5744, 252.9002, 249.7219, 265.1628,\n",
            "          265.9382, 273.9287, 277.9310, 276.1101, 283.3156, 277.9437]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[277.94366]\n",
            "tensor([[[240.5100, 228.8000, 222.9500, 243.1500, 246.7900, 261.2000, 253.4200,\n",
            "          261.6500, 257.7500, 246.1500, 251.8300, 248.1900, 264.8600, 265.1300,\n",
            "          274.0300, 278.2000, 275.6600, 283.7900, 277.7600, 279.1000]]])\n",
            "tensor([[[241.9978, 231.3969, 228.2963, 244.3334, 248.6367, 261.2863, 254.6913,\n",
            "          263.7888, 259.1422, 248.5744, 252.9002, 249.7219, 265.1628, 265.9382,\n",
            "          273.9287, 277.9310, 276.1101, 283.3156, 277.9437, 278.8433]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[278.84326]\n",
            "tensor([[[228.8000, 222.9500, 243.1500, 246.7900, 261.2000, 253.4200, 261.6500,\n",
            "          257.7500, 246.1500, 251.8300, 248.1900, 264.8600, 265.1300, 274.0300,\n",
            "          278.2000, 275.6600, 283.7900, 277.7600, 279.1000, 286.6400]]])\n",
            "tensor([[[231.3969, 227.0552, 244.1793, 248.3452, 261.2863, 254.6913, 263.7888,\n",
            "          259.1422, 248.5742, 252.8997, 249.7218, 265.1628, 265.9382, 273.9287,\n",
            "          277.9310, 276.1101, 283.3156, 277.9437, 278.8433, 285.6602]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[285.6602]\n",
            "tensor([[[222.9500, 243.1500, 246.7900, 261.2000, 253.4200, 261.6500, 257.7500,\n",
            "          246.1500, 251.8300, 248.1900, 264.8600, 265.1300, 274.0300, 278.2000,\n",
            "          275.6600, 283.7900, 277.7600, 279.1000, 286.6400, 281.5900]]])\n",
            "tensor([[[226.8637, 244.1555, 248.3002, 261.2863, 254.6913, 263.7888, 259.1422,\n",
            "          248.5452, 252.8996, 249.7218, 265.1628, 265.9382, 273.9287, 277.9310,\n",
            "          276.1101, 283.3156, 277.9437, 278.8433, 285.6602, 281.7456]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[281.7456]\n",
            "tensor([[[243.1500, 246.7900, 261.2000, 253.4200, 261.6500, 257.7500, 246.1500,\n",
            "          251.8300, 248.1900, 264.8600, 265.1300, 274.0300, 278.2000, 275.6600,\n",
            "          283.7900, 277.7600, 279.1000, 286.6400, 281.5900, 273.0400]]])\n",
            "tensor([[[244.1213, 248.1830, 261.2863, 254.6913, 263.7888, 259.1422, 248.5452,\n",
            "          252.8636, 249.7218, 265.1628, 265.9382, 273.9287, 277.9310, 276.1101,\n",
            "          283.3156, 277.9437, 278.8433, 285.6602, 281.7456, 273.6380]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[273.638]\n",
            "tensor([[[246.7900, 261.2000, 253.4200, 261.6500, 257.7500, 246.1500, 251.8300,\n",
            "          248.1900, 264.8600, 265.1300, 274.0300, 278.2000, 275.6600, 283.7900,\n",
            "          277.7600, 279.1000, 286.6400, 281.5900, 273.0400, 279.1000]]])\n",
            "tensor([[[248.1830, 261.2863, 254.6913, 263.7888, 259.1422, 248.5452, 252.8636,\n",
            "          249.6726, 265.1628, 265.9382, 273.9287, 277.9310, 276.1101, 283.3156,\n",
            "          277.9437, 278.8433, 285.6602, 281.7456, 273.6380, 278.7331]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[278.7331]\n",
            "tensor([[[261.2000, 253.4200, 261.6500, 257.7500, 246.1500, 251.8300, 248.1900,\n",
            "          264.8600, 265.1300, 274.0300, 278.2000, 275.6600, 283.7900, 277.7600,\n",
            "          279.1000, 286.6400, 281.5900, 273.0400, 279.1000, 279.0800]]])\n",
            "tensor([[[261.2863, 254.6913, 263.7888, 259.1422, 248.5452, 252.8636, 249.6726,\n",
            "          265.1628, 265.9382, 273.9287, 277.9310, 276.1101, 283.3156, 277.9437,\n",
            "          278.8433, 285.6602, 281.7456, 273.6380, 278.7331, 278.8096]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[278.8096]\n",
            "tensor([[[253.4200, 261.6500, 257.7500, 246.1500, 251.8300, 248.1900, 264.8600,\n",
            "          265.1300, 274.0300, 278.2000, 275.6600, 283.7900, 277.7600, 279.1000,\n",
            "          286.6400, 281.5900, 273.0400, 279.1000, 279.0800, 282.9700]]])\n",
            "tensor([[[254.6913, 262.3561, 259.0158, 248.2536, 252.8636, 249.6726, 265.1628,\n",
            "          265.9382, 273.9287, 277.9310, 276.1101, 283.3156, 277.9437, 278.8433,\n",
            "          285.6602, 281.7456, 273.6380, 278.7331, 278.8096, 282.2597]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[282.25974]\n",
            "tensor([[[261.6500, 257.7500, 246.1500, 251.8300, 248.1900, 264.8600, 265.1300,\n",
            "          274.0300, 278.2000, 275.6600, 283.7900, 277.7600, 279.1000, 286.6400,\n",
            "          281.5900, 273.0400, 279.1000, 279.0800, 282.9700, 287.0500]]])\n",
            "tensor([[[262.3561, 259.0158, 248.2536, 252.8636, 249.6726, 265.1628, 265.9382,\n",
            "          273.9287, 277.9310, 276.1101, 283.3156, 277.9437, 278.8433, 285.6602,\n",
            "          281.7456, 273.6380, 278.7331, 278.8096, 282.2597, 286.6181]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[286.61807]\n",
            "tensor([[[257.7500, 246.1500, 251.8300, 248.1900, 264.8600, 265.1300, 274.0300,\n",
            "          278.2000, 275.6600, 283.7900, 277.7600, 279.1000, 286.6400, 281.5900,\n",
            "          273.0400, 279.1000, 279.0800, 282.9700, 287.0500, 285.7300]]])\n",
            "tensor([[[259.0158, 248.2536, 252.8636, 249.6726, 265.1628, 265.9382, 273.9287,\n",
            "          277.9310, 276.1101, 283.3156, 277.9437, 278.8433, 285.6602, 281.7456,\n",
            "          273.6380, 278.7331, 278.8096, 282.2597, 286.6181, 285.4128]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[285.4128]\n",
            "tensor([[[246.1500, 251.8300, 248.1900, 264.8600, 265.1300, 274.0300, 278.2000,\n",
            "          275.6600, 283.7900, 277.7600, 279.1000, 286.6400, 281.5900, 273.0400,\n",
            "          279.1000, 279.0800, 282.9700, 287.0500, 285.7300, 293.2100]]])\n",
            "tensor([[[248.2536, 252.8636, 249.6726, 265.1628, 265.9382, 273.9287, 277.9310,\n",
            "          276.1101, 283.3156, 277.9437, 278.8433, 285.6602, 281.7456, 273.6380,\n",
            "          278.7331, 278.8096, 282.2597, 286.6181, 285.4128, 292.1212]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[292.12115]\n",
            "tensor([[[251.8300, 248.1900, 264.8600, 265.1300, 274.0300, 278.2000, 275.6600,\n",
            "          283.7900, 277.7600, 279.1000, 286.6400, 281.5900, 273.0400, 279.1000,\n",
            "          279.0800, 282.9700, 287.0500, 285.7300, 293.2100, 290.4800]]])\n",
            "tensor([[[252.8636, 249.6726, 265.1628, 265.9382, 273.9287, 277.9310, 276.1101,\n",
            "          283.3156, 277.9437, 278.8433, 285.6602, 281.7456, 273.6380, 278.7331,\n",
            "          278.8096, 282.2597, 286.6181, 285.4128, 292.1212, 289.7113]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[289.71133]\n",
            "tensor([[[248.1900, 264.8600, 265.1300, 274.0300, 278.2000, 275.6600, 283.7900,\n",
            "          277.7600, 279.1000, 286.6400, 281.5900, 273.0400, 279.1000, 279.0800,\n",
            "          282.9700, 287.0500, 285.7300, 293.2100, 290.4800, 282.7900]]])\n",
            "tensor([[[249.6726, 265.1628, 265.9382, 273.9287, 277.9310, 276.1101, 283.3156,\n",
            "          277.9437, 278.8433, 285.6602, 281.7456, 273.6380, 278.7331, 278.8096,\n",
            "          282.2597, 286.6181, 285.4128, 292.1212, 289.7113, 282.3922]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[282.39224]\n",
            "tensor([[[264.8600, 265.1300, 274.0300, 278.2000, 275.6600, 283.7900, 277.7600,\n",
            "          279.1000, 286.6400, 281.5900, 273.0400, 279.1000, 279.0800, 282.9700,\n",
            "          287.0500, 285.7300, 293.2100, 290.4800, 282.7900, 283.5700]]])\n",
            "tensor([[[265.1628, 265.9382, 273.9287, 277.9310, 276.1101, 283.3156, 277.9437,\n",
            "          278.8433, 285.6602, 281.7456, 273.6380, 278.7331, 278.8096, 282.2597,\n",
            "          286.6181, 285.4128, 292.1212, 289.7113, 282.3922, 283.3175]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[283.31747]\n",
            "tensor([[[265.1300, 274.0300, 278.2000, 275.6600, 283.7900, 277.7600, 279.1000,\n",
            "          286.6400, 281.5900, 273.0400, 279.1000, 279.0800, 282.9700, 287.0500,\n",
            "          285.7300, 293.2100, 290.4800, 282.7900, 283.5700, 286.1900]]])\n",
            "tensor([[[265.9382, 273.9287, 277.9310, 276.1101, 283.3156, 277.9437, 278.8433,\n",
            "          285.6602, 281.7456, 273.6380, 278.7331, 278.8096, 282.2597, 286.6181,\n",
            "          285.4128, 292.1212, 289.7113, 282.3922, 283.3175, 285.6273]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[285.6273]\n",
            "tensor([[[274.0300, 278.2000, 275.6600, 283.7900, 277.7600, 279.1000, 286.6400,\n",
            "          281.5900, 273.0400, 279.1000, 279.0800, 282.9700, 287.0500, 285.7300,\n",
            "          293.2100, 290.4800, 282.7900, 283.5700, 286.1900, 284.2500]]])\n",
            "tensor([[[273.9287, 277.9310, 276.1101, 283.3156, 277.9437, 278.8433, 285.6602,\n",
            "          281.7456, 273.6380, 278.7331, 278.8096, 282.2597, 286.6181, 285.4128,\n",
            "          292.1212, 289.7113, 282.3922, 283.3175, 285.6273, 283.7879]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[283.78793]\n",
            "tensor([[[278.2000, 275.6600, 283.7900, 277.7600, 279.1000, 286.6400, 281.5900,\n",
            "          273.0400, 279.1000, 279.0800, 282.9700, 287.0500, 285.7300, 293.2100,\n",
            "          290.4800, 282.7900, 283.5700, 286.1900, 284.2500, 287.6800]]])\n",
            "tensor([[[277.9310, 276.1101, 283.3156, 277.9437, 278.8433, 285.6602, 281.7456,\n",
            "          273.5963, 278.7331, 278.8096, 282.2597, 286.6181, 285.4128, 292.1212,\n",
            "          289.7113, 282.3922, 283.3175, 285.6273, 283.7879, 286.7819]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[286.78192]\n",
            "tensor([[[275.6600, 283.7900, 277.7600, 279.1000, 286.6400, 281.5900, 273.0400,\n",
            "          279.1000, 279.0800, 282.9700, 287.0500, 285.7300, 293.2100, 290.4800,\n",
            "          282.7900, 283.5700, 286.1900, 284.2500, 287.6800, 292.4400]]])\n",
            "tensor([[[276.1101, 283.3156, 277.9437, 278.8433, 285.6602, 281.7456, 273.5963,\n",
            "          278.7331, 278.8096, 282.2597, 286.6181, 285.4128, 292.1212, 289.7113,\n",
            "          282.3922, 283.3175, 285.6273, 283.7879, 286.7819, 291.1854]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[291.18536]\n",
            "tensor([[[283.7900, 277.7600, 279.1000, 286.6400, 281.5900, 273.0400, 279.1000,\n",
            "          279.0800, 282.9700, 287.0500, 285.7300, 293.2100, 290.4800, 282.7900,\n",
            "          283.5700, 286.1900, 284.2500, 287.6800, 292.4400, 292.5000]]])\n",
            "tensor([[[283.3156, 277.9437, 278.8433, 285.6602, 281.7456, 273.5963, 278.7331,\n",
            "          278.8096, 282.2597, 286.6181, 285.4128, 292.1212, 289.7113, 282.3922,\n",
            "          283.3175, 285.6273, 283.7879, 286.7819, 291.1854, 291.8793]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[291.8793]\n",
            "tensor([[[277.7600, 279.1000, 286.6400, 281.5900, 273.0400, 279.1000, 279.0800,\n",
            "          282.9700, 287.0500, 285.7300, 293.2100, 290.4800, 282.7900, 283.5700,\n",
            "          286.1900, 284.2500, 287.6800, 292.4400, 292.5000, 286.6700]]])\n",
            "tensor([[[277.9437, 278.8433, 285.6602, 281.7456, 273.5963, 278.7331, 278.8096,\n",
            "          282.2597, 286.6181, 285.4128, 292.1212, 289.7113, 282.3922, 283.3175,\n",
            "          285.6273, 283.7879, 286.7819, 291.1854, 291.8793, 286.2807]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[286.28073]\n",
            "tensor([[[279.1000, 286.6400, 281.5900, 273.0400, 279.1000, 279.0800, 282.9700,\n",
            "          287.0500, 285.7300, 293.2100, 290.4800, 282.7900, 283.5700, 286.1900,\n",
            "          284.2500, 287.6800, 292.4400, 292.5000, 286.6700, 281.6000]]])\n",
            "tensor([[[278.8433, 285.6602, 281.7456, 273.5963, 278.7331, 278.8096, 282.2597,\n",
            "          286.6181, 285.4128, 292.1212, 289.7113, 282.3922, 283.3175, 285.6273,\n",
            "          283.7879, 286.7819, 291.1854, 291.8793, 286.2807, 281.2715]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[281.2715]\n",
            "tensor([[[286.6400, 281.5900, 273.0400, 279.1000, 279.0800, 282.9700, 287.0500,\n",
            "          285.7300, 293.2100, 290.4800, 282.7900, 283.5700, 286.1900, 284.2500,\n",
            "          287.6800, 292.4400, 292.5000, 286.6700, 281.6000, 284.9700]]])\n",
            "tensor([[[285.6602, 281.7456, 273.5963, 278.7331, 278.8096, 282.2597, 286.6181,\n",
            "          285.4128, 292.1212, 289.7113, 282.3922, 283.3175, 285.6273, 283.7879,\n",
            "          286.7819, 291.1854, 291.8793, 286.2807, 281.2715, 284.1281]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[284.12808]\n",
            "tensor([[[281.5900, 273.0400, 279.1000, 279.0800, 282.9700, 287.0500, 285.7300,\n",
            "          293.2100, 290.4800, 282.7900, 283.5700, 286.1900, 284.2500, 287.6800,\n",
            "          292.4400, 292.5000, 286.6700, 281.6000, 284.9700, 286.2800]]])\n",
            "tensor([[[281.7456, 273.5963, 278.7331, 278.8096, 282.2597, 286.6181, 285.4128,\n",
            "          292.1212, 289.7113, 282.3922, 283.3175, 285.6273, 283.7879, 286.7819,\n",
            "          291.1854, 291.8793, 286.2807, 281.2715, 284.1281, 285.2990]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[285.29904]\n",
            "tensor([[[273.0400, 279.1000, 279.0800, 282.9700, 287.0500, 285.7300, 293.2100,\n",
            "          290.4800, 282.7900, 283.5700, 286.1900, 284.2500, 287.6800, 292.4400,\n",
            "          292.5000, 286.6700, 281.6000, 284.9700, 286.2800, 295.0000]]])\n",
            "tensor([[[273.5963, 278.7331, 278.8096, 282.2597, 286.6181, 285.4128, 292.1212,\n",
            "          289.7113, 282.3922, 283.3175, 285.6273, 283.7879, 286.7819, 291.1854,\n",
            "          291.8793, 286.2807, 281.2715, 284.1281, 285.2990, 293.9306]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[293.93057]\n",
            "tensor([[[279.1000, 279.0800, 282.9700, 287.0500, 285.7300, 293.2100, 290.4800,\n",
            "          282.7900, 283.5700, 286.1900, 284.2500, 287.6800, 292.4400, 292.5000,\n",
            "          286.6700, 281.6000, 284.9700, 286.2800, 295.0000, 291.9700]]])\n",
            "tensor([[[278.7331, 278.8096, 282.2597, 286.6181, 285.4128, 292.1212, 289.7113,\n",
            "          282.3558, 283.3175, 285.6273, 283.7879, 286.7819, 291.1854, 291.8793,\n",
            "          286.2807, 281.2715, 284.1281, 285.2990, 293.9306, 291.2496]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[291.2496]\n",
            "tensor([[[279.0800, 282.9700, 287.0500, 285.7300, 293.2100, 290.4800, 282.7900,\n",
            "          283.5700, 286.1900, 284.2500, 287.6800, 292.4400, 292.5000, 286.6700,\n",
            "          281.6000, 284.9700, 286.2800, 295.0000, 291.9700, 296.9300]]])\n",
            "tensor([[[278.8096, 282.2597, 286.6181, 285.4128, 292.1212, 289.7113, 282.3558,\n",
            "          283.2985, 285.6273, 283.7879, 286.7819, 291.1854, 291.8793, 286.2807,\n",
            "          281.2715, 284.1281, 285.2990, 293.9306, 291.2496, 295.6366]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[295.63657]\n",
            "tensor([[[282.9700, 287.0500, 285.7300, 293.2100, 290.4800, 282.7900, 283.5700,\n",
            "          286.1900, 284.2500, 287.6800, 292.4400, 292.5000, 286.6700, 281.6000,\n",
            "          284.9700, 286.2800, 295.0000, 291.9700, 296.9300, 294.8800]]])\n",
            "tensor([[[282.2597, 286.6181, 285.4128, 292.1212, 289.7113, 282.3558, 283.2985,\n",
            "          285.6273, 283.7879, 286.7819, 291.1854, 291.8793, 286.2807, 281.2715,\n",
            "          284.1281, 285.2990, 293.9306, 291.2496, 295.6366, 293.7438]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[293.7438]\n",
            "tensor([[[287.0500, 285.7300, 293.2100, 290.4800, 282.7900, 283.5700, 286.1900,\n",
            "          284.2500, 287.6800, 292.4400, 292.5000, 286.6700, 281.6000, 284.9700,\n",
            "          286.2800, 295.0000, 291.9700, 296.9300, 294.8800, 295.4400]]])\n",
            "tensor([[[286.6181, 285.4128, 292.1212, 289.7113, 282.3558, 283.2985, 285.6273,\n",
            "          283.7879, 286.7819, 291.1854, 291.8793, 286.2807, 281.2715, 284.1281,\n",
            "          285.2990, 293.9306, 291.2496, 295.6366, 293.7438, 294.0792]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[294.07922]\n",
            "tensor([[[285.7300, 293.2100, 290.4800, 282.7900, 283.5700, 286.1900, 284.2500,\n",
            "          287.6800, 292.4400, 292.5000, 286.6700, 281.6000, 284.9700, 286.2800,\n",
            "          295.0000, 291.9700, 296.9300, 294.8800, 295.4400, 299.0800]]])\n",
            "tensor([[[285.4128, 292.1212, 289.7113, 282.3558, 283.2985, 285.6273, 283.7879,\n",
            "          286.7819, 291.1854, 291.8793, 286.2807, 281.2715, 284.1281, 285.2990,\n",
            "          293.9306, 291.2496, 295.6366, 293.7438, 294.0792, 297.8483]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[297.84827]\n",
            "tensor([[[293.2100, 290.4800, 282.7900, 283.5700, 286.1900, 284.2500, 287.6800,\n",
            "          292.4400, 292.5000, 286.6700, 281.6000, 284.9700, 286.2800, 295.0000,\n",
            "          291.9700, 296.9300, 294.8800, 295.4400, 299.0800, 303.5300]]])\n",
            "tensor([[[292.1212, 289.7113, 282.3558, 283.2985, 285.6273, 283.7879, 286.7819,\n",
            "          291.1854, 291.8793, 286.2807, 281.2715, 284.1281, 285.2990, 293.9306,\n",
            "          291.2496, 295.6366, 293.7438, 294.0792, 297.8483, 301.9845]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[301.9845]\n",
            "tensor([[[290.4800, 282.7900, 283.5700, 286.1900, 284.2500, 287.6800, 292.4400,\n",
            "          292.5000, 286.6700, 281.6000, 284.9700, 286.2800, 295.0000, 291.9700,\n",
            "          296.9300, 294.8800, 295.4400, 299.0800, 303.5300, 302.9700]]])\n",
            "tensor([[[289.7113, 282.3558, 283.2985, 285.6273, 283.7879, 286.7819, 291.1854,\n",
            "          291.8793, 286.2807, 281.2715, 284.1281, 285.2990, 293.9306, 291.2496,\n",
            "          295.6366, 293.7438, 294.0792, 297.8483, 301.9845, 301.4725]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[301.47253]\n",
            "tensor([[[282.7900, 283.5700, 286.1900, 284.2500, 287.6800, 292.4400, 292.5000,\n",
            "          286.6700, 281.6000, 284.9700, 286.2800, 295.0000, 291.9700, 296.9300,\n",
            "          294.8800, 295.4400, 299.0800, 303.5300, 302.9700, 304.3200]]])\n",
            "tensor([[[282.3558, 283.2985, 285.6273, 283.7879, 286.7819, 291.1854, 291.8793,\n",
            "          286.2807, 281.2715, 284.1281, 285.2990, 293.9306, 291.2496, 295.6366,\n",
            "          293.7438, 294.0792, 297.8483, 301.9845, 301.4725, 302.5933]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[302.59326]\n",
            "tensor([[[283.5700, 286.1900, 284.2500, 287.6800, 292.4400, 292.5000, 286.6700,\n",
            "          281.6000, 284.9700, 286.2800, 295.0000, 291.9700, 296.9300, 294.8800,\n",
            "          295.4400, 299.0800, 303.5300, 302.9700, 304.3200, 305.5500]]])\n",
            "tensor([[[283.2985, 285.6273, 283.7879, 286.7819, 291.1854, 291.8793, 286.2807,\n",
            "          281.2274, 284.1281, 285.2990, 293.9306, 291.2496, 295.6366, 293.7438,\n",
            "          294.0792, 297.8483, 301.9845, 301.4725, 302.5933, 304.2623]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[304.26227]\n",
            "tensor([[[286.1900, 284.2500, 287.6800, 292.4400, 292.5000, 286.6700, 281.6000,\n",
            "          284.9700, 286.2800, 295.0000, 291.9700, 296.9300, 294.8800, 295.4400,\n",
            "          299.0800, 303.5300, 302.9700, 304.3200, 305.5500, 308.0800]]])\n",
            "tensor([[[285.6273, 283.7879, 286.7819, 291.1854, 291.8793, 286.2807, 281.2274,\n",
            "          284.0804, 285.2990, 293.9306, 291.2496, 295.6366, 293.7438, 294.0792,\n",
            "          297.8483, 301.9845, 301.4725, 302.5933, 304.2623, 306.5422]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[306.54218]\n",
            "tensor([[[284.2500, 287.6800, 292.4400, 292.5000, 286.6700, 281.6000, 284.9700,\n",
            "          286.2800, 295.0000, 291.9700, 296.9300, 294.8800, 295.4400, 299.0800,\n",
            "          303.5300, 302.9700, 304.3200, 305.5500, 308.0800, 312.1800]]])\n",
            "tensor([[[283.7879, 286.7819, 291.1854, 291.8793, 286.2807, 281.2274, 284.0804,\n",
            "          285.2990, 293.9306, 291.2496, 295.6366, 293.7438, 294.0792, 297.8483,\n",
            "          301.9845, 301.4725, 302.5933, 304.2623, 306.5422, 310.3186]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[310.31857]\n",
            "tensor([[[287.6800, 292.4400, 292.5000, 286.6700, 281.6000, 284.9700, 286.2800,\n",
            "          295.0000, 291.9700, 296.9300, 294.8800, 295.4400, 299.0800, 303.5300,\n",
            "          302.9700, 304.3200, 305.5500, 308.0800, 312.1800, 311.3600]]])\n",
            "tensor([[[286.7819, 291.1854, 291.8793, 286.2807, 281.2274, 284.0804, 285.2990,\n",
            "          293.9306, 291.2496, 295.6366, 293.7438, 294.0792, 297.8483, 301.9845,\n",
            "          301.4725, 302.5933, 304.2623, 306.5422, 310.3186, 309.5269]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[309.52692]\n",
            "tensor([[[292.4400, 292.5000, 286.6700, 281.6000, 284.9700, 286.2800, 295.0000,\n",
            "          291.9700, 296.9300, 294.8800, 295.4400, 299.0800, 303.5300, 302.9700,\n",
            "          304.3200, 305.5500, 308.0800, 312.1800, 311.3600, 319.3400]]])\n",
            "tensor([[[291.1854, 291.8793, 286.2807, 281.2274, 284.0804, 285.2990, 293.9306,\n",
            "          291.2496, 295.6366, 293.7438, 294.0792, 297.8483, 301.9845, 301.4725,\n",
            "          302.5933, 304.2623, 306.5422, 310.3186, 309.5269, 316.7875]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[316.78748]\n",
            "tensor([[[292.5000, 286.6700, 281.6000, 284.9700, 286.2800, 295.0000, 291.9700,\n",
            "          296.9300, 294.8800, 295.4400, 299.0800, 303.5300, 302.9700, 304.3200,\n",
            "          305.5500, 308.0800, 312.1800, 311.3600, 319.3400, 323.2000]]])\n",
            "tensor([[[291.8793, 286.2807, 281.2274, 284.0804, 285.2990, 293.9306, 291.2496,\n",
            "          295.6366, 293.7438, 294.0792, 297.8483, 301.9845, 301.4725, 302.5933,\n",
            "          304.2623, 306.5422, 310.3186, 309.5269, 316.7875, 321.0779]]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "[321.0779]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "oJK_20erlrT3",
        "outputId": "3ad3955a-37d9-4215-8d43-de98402881ab"
      },
      "source": [
        "# last forecast_size sample, here these are the actual values of t+1,t+2... \n",
        "#assuming sliding window style\n",
        "y_pred_actual = forecast['close'].tail(X_fore.shape[0])\n",
        "\n",
        "plt.plot(y_pred_actual.index,y_pred_actual,label = 'Actual')\n",
        "plt.plot(y_pred_actual.index,forecasted,label = 'Predicted')\n",
        "plt.legend()\n",
        "print('Actual: ',y_pred_actual.to_numpy())\n",
        "print('Forecasted: ',forecasted )\n",
        "\n",
        "forecasted_loss = mean_squared_error(forecasted,y_pred_actual.to_numpy())\n",
        "print('MSE Loss for forecasted value(s): ', forecasted_loss)\n",
        "print('Loss on the Dollar: ', np.sum(y_pred_actual.to_numpy() - forecasted))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual:  [253.42 261.65 257.75 246.15 251.83 248.19 264.86 265.13 274.03 278.2\n",
            " 275.66 283.79 277.76 279.1  286.64 281.59 273.04 279.1  279.08 282.97\n",
            " 287.05 285.73 293.21 290.48 282.79 283.57 286.19 284.25 287.68 292.44\n",
            " 292.5  286.67 281.6  284.97 286.28 295.   291.97 296.93 294.88 295.44\n",
            " 299.08 303.53 302.97 304.32 305.55 308.08 312.18 311.36 319.34 323.2\n",
            " 320.79]\n",
            "Forecasted:  [261.28625, 254.69133, 263.78885, 259.1422, 248.57436, 252.90025, 249.72186, 265.16284, 265.93817, 273.92874, 277.93103, 276.1101, 283.31558, 277.94366, 278.84326, 285.6602, 281.7456, 273.638, 278.7331, 278.8096, 282.25974, 286.61807, 285.4128, 292.12115, 289.71133, 282.39224, 283.31747, 285.6273, 283.78793, 286.78192, 291.18536, 291.8793, 286.28073, 281.2715, 284.12808, 285.29904, 293.93057, 291.2496, 295.63657, 293.7438, 294.07922, 297.84827, 301.9845, 301.47253, 302.59326, 304.26227, 306.54218, 310.31857, 309.52692, 316.78748, 321.0779]\n",
            "MSE Loss for forecasted value(s):  31.659204277557887\n",
            "Loss on the Dollar:  82.97733947753918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUHaSG5ylziP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}